{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Ir-eqDndOzv"
      },
      "source": [
        "# Trend prediction for scientific concepts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lIp5ttz7Bag0"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy import sparse, special\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from datetime import date, timedelta, datetime\n",
        "import time\n",
        "import requests\n",
        "# random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "j9aMUnnZ2m1a"
      },
      "outputs": [],
      "source": [
        "#   Training args   #\n",
        "#######################\n",
        "SEQSIZE = 3\n",
        "\n",
        "RATIO = 0.5 # training 0/1 ratio\n",
        "\n",
        "NN_BATCHSIZE = 500\n",
        "EPOCHS = 10\n",
        "CUTOFFTYPE = 'undirctf'#  ['dirctf,undirctf']\n",
        "\n",
        "#   Preprocessing   #\n",
        "##################\n",
        "NUM_OF_VERTICES=64719 # number of vertices of the semantic net\n",
        "VERBOSE = 2\n",
        "\n",
        "number_of_years = 3\n",
        "number_of_months = 1\n",
        "time_snapshots = 3\n",
        "\n",
        "raw_data_connected_ratio = 0.5\n",
        "\n",
        "EDGES_USED = 1*(10**6)\n",
        "FILTERUNCONNECTED = True\n",
        "TESTSIZE = 1*(10**3)\n",
        "\n",
        "NUM_OF_FEATURES = 5\n",
        "\n",
        "BATCH_SIZE = 1*(10**6)\n",
        "ratio = 0.5\n",
        "train_test_connected_number = []\n",
        "\n",
        "rndidtest = np.random.permutation(range(TESTSIZE))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset names\n",
        "\n",
        "Please download the datasets in a zip file from the URL below, and extract the *.pkl files in the root directory of this notebook."
      ],
      "metadata": {
        "id": "qK16d1WgfF4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_names = ['SemanticGraph_delta_1_cutoff_0_minedge_1.pkl',\n",
        "'SemanticGraph_delta_1_cutoff_0_minedge_3.pkl',\n",
        "'SemanticGraph_delta_1_cutoff_5_minedge_1.pkl',\n",
        "'SemanticGraph_delta_1_cutoff_5_minedge_3.pkl',\n",
        "'SemanticGraph_delta_1_cutoff_25_minedge_1.pkl',\n",
        "'SemanticGraph_delta_1_cutoff_25_minedge_3.pkl',\n",
        "'SemanticGraph_delta_3_cutoff_0_minedge_1.pkl',\n",
        "'SemanticGraph_delta_3_cutoff_0_minedge_3.pkl',\n",
        "'SemanticGraph_delta_3_cutoff_5_minedge_1.pkl',\n",
        "'SemanticGraph_delta_3_cutoff_5_minedge_3.pkl',\n",
        "'SemanticGraph_delta_3_cutoff_25_minedge_1.pkl',\n",
        "'SemanticGraph_delta_3_cutoff_25_minedge_3.pkl',\n",
        "'SemanticGraph_delta_5_cutoff_0_minedge_1.pkl',\n",
        "'SemanticGraph_delta_5_cutoff_0_minedge_3.pkl',\n",
        "'SemanticGraph_delta_5_cutoff_5_minedge_1.pkl',\n",
        "'SemanticGraph_delta_5_cutoff_5_minedge_3.pkl',\n",
        "'SemanticGraph_delta_5_cutoff_25_minedge_1.pkl',\n",
        "'SemanticGraph_delta_5_cutoff_25_minedge_3.pkl']"
      ],
      "metadata": {
        "id": "nY_DIvPLct3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCjDfI4lamxG"
      },
      "outputs": [],
      "source": [
        "def get_undir(matrix):\n",
        "    return matrix + matrix.T\n",
        "\n",
        "def get_matrix(pairs, number_of_vertices = 64719, convert_to_undirected = True):\n",
        "    matrix = sparse.csr_matrix((np.ones(len(pairs)), (pairs[:,0], pairs[:,1])), shape=(number_of_vertices,number_of_vertices))\n",
        "    if convert_to_undirected:\n",
        "        matrix = get_undir(matrix)\n",
        "    return matrix\n",
        "\n",
        "def get_degs(matrix, convert_to_undirected = True):\n",
        "    if convert_to_undirected:\n",
        "        matrix = get_undir(matrix)\n",
        "    return np.array(matrix.sum(0))[0]\n",
        "\n",
        "def get_percentiles(arr, percentile = 1):\n",
        "    percentiles = np.percentile(arr, np.arange(0,100,percentile))\n",
        "    # print(percentiles)\n",
        "    return percentiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lIMNZYXNxRn"
      },
      "outputs": [],
      "source": [
        "VERBOSE = 0\n",
        "def print_progress(print_output, importance_rank = 2, verbose = VERBOSE):\n",
        "    '''\n",
        "    The entire runtime receives a fixed VERBOSE value\n",
        "    Prints the info while executing:\n",
        "    2: Every thing\n",
        "    1: Only completion of each stage.\n",
        "    0: Nothing'''\n",
        "    if importance_rank <= verbose:\n",
        "        if type(print_output) == tuple:\n",
        "            print(*print_output)\n",
        "        else:\n",
        "            print(print_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OKYJ9xcdO0K"
      },
      "source": [
        "<a id='train'></a>\n",
        "# 3. Creating historic training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G8sk58gdO0L",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def create_training_data(full_graph,year_start,years_delta,filter_data = True, edges_used=100000,\n",
        "                         vertex_degree_cutoff=10, connected_ratio = 0.5, return_graphs = False, unique_samples = False, weight = 3):\n",
        "\n",
        "    # day_curr=date(2014,12,31)\n",
        "    day_origin = date(1990,1,1)\n",
        "    years=[year_start,year_start+years_delta]    \n",
        "    \n",
        "    all_G=[]\n",
        "    all_edge_lists=[]\n",
        "    all_sparse=[]\n",
        "    for yy in years:\n",
        "        print_progress(f'years_delta create: {years_delta}', 1, VERBOSE)\n",
        "        print_progress(('Create Graph for ', yy),2, VERBOSE)\n",
        "        day_curr=date(yy,12,31)\n",
        "        # print('    All the edges before ', day_curr)\n",
        "        all_edges_curr=full_graph[full_graph[:,2]<(day_curr-day_origin).days]\n",
        "        adj_mat_sparse_curr = sparse.csr_matrix((np.ones(len(all_edges_curr)), (all_edges_curr[:,0], all_edges_curr[:,1])), shape=(NUM_OF_VERTICES,NUM_OF_VERTICES))\n",
        "        G_curr=nx.from_scipy_sparse_matrix(adj_mat_sparse_curr, parallel_edges=False, create_using=None, edge_attribute='weight')\n",
        "\n",
        "        all_G.append(G_curr)\n",
        "        all_sparse.append(adj_mat_sparse_curr)\n",
        "        all_edge_lists.append(all_edges_curr)\n",
        "\n",
        "        print_progress(('    Done: Create Graph for ', yy),2, VERBOSE)\n",
        "        print_progress(('    num of edges: ', G_curr.number_of_edges()),2, VERBOSE)\n",
        "\n",
        "\n",
        "    if CUTOFFTYPE == 'dirctf':\n",
        "        all_degs=np.array(all_sparse[0].sum(0))[0]\n",
        "    ######## \n",
        "    # Modify degree calculation to adapt undirected graph\n",
        "    if CUTOFFTYPE == 'undirctf':\n",
        "        all_degs=np.array((all_sparse[0]+all_sparse[0].T).sum(0))[0]\n",
        "\n",
        "\n",
        "    ## Create all edges to be predicted\n",
        "    all_vertices=np.array(range(NUM_OF_VERTICES))\n",
        "\n",
        "    unconnected_vertex_pairs=[]\n",
        "    unconnected_vertex_pairs_solution=[]\n",
        "# filtering the pairs by degree cutoff and ration of connected to unconnected\n",
        "    if filter_data == True:\n",
        "        large_deg_pairs = []\n",
        "        for pair in all_edge_lists[1][:,0:2]: #full_graph[:,0:2]: >>> debugged for faster execution 20-11-2021\n",
        "            if all_degs[pair[0]]>=vertex_degree_cutoff and all_degs[pair[1]]>=vertex_degree_cutoff:\n",
        "                large_deg_pairs.append(pair)\n",
        "        large_deg_pairs = np.array(large_deg_pairs)\n",
        "        print_progress((f'    Cutoff size: {vertex_degree_cutoff}'),2, VERBOSE)       \n",
        "        print_progress((f'    Potential pairs after cutoff: {large_deg_pairs.shape[0]}'),2, VERBOSE)\n",
        "\n",
        "        vertex_large_degs=all_vertices[all_degs>=vertex_degree_cutoff] # use only vertices with degrees larger than 10.\n",
        "\n",
        "        ################\n",
        "        # enable or diable the unique edges here\n",
        "        if unique_samples:\n",
        "            large_deg_pairs = np.unique(large_deg_pairs, axis=0)\n",
        "        print_progress((f'    Number of pairs: {large_deg_pairs.shape[0]}'),2, VERBOSE)\n",
        "        \n",
        "        # Shuffle the large deg pairs\n",
        "        idxrnd = np.random.permutation(np.arange(len(large_deg_pairs)))\n",
        "        large_deg_pairs = large_deg_pairs[idxrnd]\n",
        "\n",
        "        time_start=time.time()\n",
        "\n",
        "        for pair in large_deg_pairs:\n",
        "            v1,v2 = pair[0], pair[1]\n",
        "\n",
        "            edge_current = all_G[0].has_edge(v1,v2) # 0/1 (Boolean like): if v1 and v2 are connected now\n",
        "          \n",
        "            edge_sol = (all_G[1].get_edge_data(v1,v2)['weight'] > (weight-1)) if all_G[1].has_edge(v1, v2) else False\n",
        "\n",
        "            if v1!=v2 and not edge_current:\n",
        "\n",
        "                    ###### Filter unconnected pairs out \n",
        "                if FILTERUNCONNECTED == True:\n",
        "                    if edge_sol: #(not edge_sol and random.random()<0.005) or :\n",
        "                        unconnected_vertex_pairs.append((v1,v2))\n",
        "                        unconnected_vertex_pairs_solution.append(edge_sol)\n",
        "\n",
        "                        if len(unconnected_vertex_pairs)%10**6==0:\n",
        "                            time_end=time.time()\n",
        "                            print_progress(('    edge progress (',time_end-time_start,'sec): ',len(unconnected_vertex_pairs)/10**6,'M/',edges_used/10**6,'M'),2, VERBOSE)\n",
        "                            time_start=time.time()\n",
        "                        if len(unconnected_vertex_pairs)>= (edges_used*connected_ratio):\n",
        "                            break\n",
        "                else:\n",
        "                    unconnected_vertex_pairs.append((v1,v2))\n",
        "                    unconnected_vertex_pairs_solution.append(edge_sol)\n",
        "                    if len(unconnected_vertex_pairs)%10**6==0:\n",
        "                        time_end=time.time()\n",
        "                        print_progress(('    edge progress (',time_end-time_start,'sec): ',len(unconnected_vertex_pairs)/10**6,'M/',edges_used/10**6,'M'),2, VERBOSE)\n",
        "                        time_start=time.time()\n",
        "        ##### generating negative sample (unconnected at end year)\n",
        "        train_0_samples = []\n",
        "        train_0_samples_solutions = []\n",
        "        vertex_large_degs_list = [v for v in vertex_large_degs]\n",
        "\n",
        "        while len(train_0_samples) < edges_used*(1-connected_ratio): #len(unconnected_vertex_pairs):\n",
        "            v1,v2=random.sample(vertex_large_degs_list, 2)\n",
        "            if not all_G[0].has_edge(v1,v2) and ((all_G[1].get_edge_data(v1,v2)['weight'] < weight) if all_G[1].has_edge(v1, v2) else True):\n",
        "                train_0_samples.append([v1,v2])\n",
        "                train_0_samples_solutions.append(0)\n",
        "                if len(train_0_samples)%10**6==0:\n",
        "                    print_progress((f'Negative sample size: {len(train_0_samples)}') ,2 , VERBOSE)                    \n",
        "        unconnected_vertex_pairs = unconnected_vertex_pairs + train_0_samples\n",
        "        unconnected_vertex_pairs_solution = unconnected_vertex_pairs_solution + train_0_samples_solutions\n",
        "    #######\n",
        "    # end of filtered data selection\n",
        "    if filter_data == False:\n",
        "        vertex_large_degs=all_vertices[all_degs>=vertex_degree_cutoff] # use only vertices with degrees larger than 10.\n",
        "        vertex_large_degs_list = [v for v in vertex_large_degs]\n",
        "        all_vertices_list = [v for v in all_vertices]\n",
        "        while len(unconnected_vertex_pairs) < edges_used: #len(unconnected_vertex_pairs):\n",
        "            v1,v2=random.sample(vertex_large_degs_list, 2)\n",
        "\n",
        "            if v1!=v2 and not all_G[0].has_edge(v1,v2):\n",
        "                unconnected_vertex_pairs.append((v1,v2))\n",
        "                temp_sol = (all_G[1].get_edge_data(v1,v2)['weight'] > (weight-1)) if all_G[1].has_edge(v1, v2) else False\n",
        "                unconnected_vertex_pairs_solution.append(temp_sol)\n",
        "\n",
        "    #########\n",
        "    # end of no filter data selection   \n",
        "    print_progress(f'Data of year: {year_start}',1,VERBOSE)\n",
        "    print_progress(('Number of unconnected vertex pairs for prediction: ', len(unconnected_vertex_pairs_solution)) ,1, VERBOSE)\n",
        "    print_progress(('Number of vertex pairs that will be connected: ' , sum(unconnected_vertex_pairs_solution)) ,1, VERBOSE)\n",
        "    print_progress(('Ratio of vertex pairs that will be connected: ' , sum(unconnected_vertex_pairs_solution)/len(unconnected_vertex_pairs_solution)),1, VERBOSE)\n",
        "    \n",
        "    unconnected_vertex_pairs=np.array(unconnected_vertex_pairs, dtype= 'int32')\n",
        "    unconnected_vertex_pairs_solution=np.array(list(map(int, unconnected_vertex_pairs_solution)))\n",
        "    all_edge_lists=all_edge_lists[0]\n",
        "    \n",
        "    if return_graphs:\n",
        "        return all_edge_lists, unconnected_vertex_pairs, unconnected_vertex_pairs_solution, all_G\n",
        "    else:\n",
        "        return all_edge_lists, unconnected_vertex_pairs, unconnected_vertex_pairs_solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q7CMHzadO0O"
      },
      "source": [
        "<a id='features'></a>\n",
        "## 4.2 Creating features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNZuFrfq4goE"
      },
      "source": [
        "# Matrix break down"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vkvqm53s8MBx"
      },
      "outputs": [],
      "source": [
        "def calculate_v_secs(sparse_mat, sorted_v, sec_number = 1000):\n",
        "    '''\n",
        "    Calculates the start and end indices of each section of v_list that corresponds to a section of matrix in the divided matrix\n",
        "    '''\n",
        "    mat_size = sparse_mat.shape[0]\n",
        "    sec_width = mat_size//sec_number\n",
        "\n",
        "    if mat_size % sec_width != 0:\n",
        "      list_lims = np.empty([sec_number+1,2], dtype = int)\n",
        "    else:\n",
        "      list_lims = np.empty([sec_number,2], dtype = int)\n",
        "\n",
        "    list_lims[:] = -1 # np.nan\n",
        "\n",
        "    for sec_index in range(sec_number):\n",
        "      sec_start = sec_index*sec_width\n",
        "      sec_end = sec_start + sec_width\n",
        "      if sec_start <= sorted_v[:,0].max():\n",
        "        list_sec_lims = np.where(np.logical_and(sorted_v[:,0]>=sec_start, sorted_v[:,0]<sec_end))[0] \n",
        "        if list_sec_lims.size !=0:\n",
        "          list_lims[sec_index,0] = list_sec_lims[0]\n",
        "          # By adding a \"1\" end itself is not included. As in v[a:b], last selected index is b-1\n",
        "          list_lims[sec_index,1] = list_sec_lims[-1]+1\n",
        "        else:\n",
        "          list_lims[sec_index,0] = -1 # np.nan\n",
        "          list_lims[sec_index,1] = -1 # np.nan\n",
        "\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    if mat_size % sec_width != 0:\n",
        "      sec_start = sec_number*sec_width\n",
        "      sec_end = mat_size # incorrect <<< sorted_v[:,0].max()\n",
        "      if sec_start <= sorted_v[:,0].max():\n",
        "        list_sec_lims = np.where(np.logical_and(sorted_v[:,0]>=sec_start, sorted_v[:,0]<sec_end))[0] \n",
        "        if list_sec_lims.size !=0:\n",
        "          list_lims[sec_number,0] = list_sec_lims[0]\n",
        "          list_lims[sec_number,1] = list_sec_lims[-1]+1\n",
        "    list_lims = np.array(list_lims)\n",
        "    # indices of sorted vlist, sorted vlist, sorting back indices\n",
        "    #sort_index\n",
        "    print_progress(('Vlist limits calculated.') ,2, VERBOSE)\n",
        "    return list_lims"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S9SLh_rYOqEJ"
      },
      "outputs": [],
      "source": [
        "# breaks matrix to row sections\n",
        "\n",
        "def matrix_sec(sparse_mat, vlist, sec_number=1000):\n",
        "    # there might be a small remaining sec at the end so the total secs will be sec_number + 1\n",
        "    sort_index = np.argsort(vlist[:,0])\n",
        "    sorted_v = vlist[sort_index]\n",
        "    sort_back = np.argsort(sort_index)\n",
        "\n",
        "    v_list_lims = calculate_v_secs(sparse_mat,sorted_v, sec_number)\n",
        "\n",
        "    mat_size = sparse_mat.shape[0]\n",
        "    FR = np.array(sparse_mat.sum(0))[0]\n",
        "    FF = []\n",
        "    MF = []\n",
        "\n",
        "\n",
        "    sec_width = mat_size//sec_number\n",
        "    if mat_size % sec_width > 0:\n",
        "        total_sec_number = sec_number + 1\n",
        "    else:\n",
        "      total_sec_number = sec_number\n",
        "    \n",
        "    for sec_index in range(total_sec_number):\n",
        "\n",
        "        start_row = sec_index*sec_width\n",
        "        if sec_index < total_sec_number - 1:\n",
        "            end_row = start_row + sec_width\n",
        "        else:\n",
        "            end_row = mat_size\n",
        "\n",
        "        mat_sec = sparse_mat[start_row:end_row]\n",
        "        mat_square_sec = mat_sec*sparse_mat\n",
        "        # FF_sec = mat_square_sec.sum(1)  debugged version below\n",
        "        FF_sec = mat_square_sec.sum(0)\n",
        "        FF.append(FF_sec)      \n",
        "\n",
        "        # Checking if the section indices exists in vlist (sorted vlist)\n",
        "\n",
        "        if v_list_lims[sec_index,0] > -1:\n",
        "            for index, pair in enumerate(sorted_v[v_list_lims[sec_index,0] : v_list_lims[sec_index,1]]):\n",
        "                MF.append(mat_square_sec[pair[0]-sec_index*sec_width,pair[1]])\n",
        "        del(mat_square_sec)\n",
        "\n",
        "        if mat_size % sec_width != 0:\n",
        "            start_row = sec_number*sec_width\n",
        "            end_row = mat_size         \n",
        "    \n",
        "    # FF = np.concatenate([np.array(sec) for sec in FF])   debugged version is below\n",
        "    FF = np.concatenate([np.array(sec) for sec in FF], axis = 0).sum(axis=0)\n",
        "    FF = np.array(FF)\n",
        "    FF = FF.reshape(FF.shape[0],)\n",
        "\n",
        "    # Sorting MF back to original order\n",
        "    MF = np.array(MF)\n",
        "    MF = MF[sort_back]\n",
        "    print_progress(('FR, FF, MF calculated.') ,2, VERBOSE)\n",
        "    return(FR, FF, MF)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcVkdpzI4Muw"
      },
      "outputs": [],
      "source": [
        "# Calculating graph features - matrix break-down is used due to memory usage limits\n",
        "def compute_all_properties(all_sparse,FR,FF,CLE,MF,ii,v1,v2):\n",
        "    \"\"\"\n",
        "    Computes hand-crafted properties for one vertex in vlist\n",
        "    \"\"\"\n",
        "    all_properties=[]\n",
        "    for i in range(len(FR)):\n",
        "        \n",
        "        all_properties.append(FR[i][v1])\n",
        "        all_properties.append(FR[i][v2])\n",
        "        all_properties.append(FF[i][v1])\n",
        "        all_properties.append(FF[i][v2])\n",
        "\n",
        "        all_properties.append(MF[i][ii])\n",
        "\n",
        "    return all_properties\n",
        "\n",
        "def compute_all_properties_of_list(all_sparse,vlist, minmaxscale=True):\n",
        "\n",
        "    time_start=time.time()\n",
        "    FR = []\n",
        "    FF = []\n",
        "    CLE = []\n",
        "    MF = []\n",
        "\n",
        "    for index, mat_snapshot in enumerate(all_sparse):\n",
        "        mat = mat_snapshot + mat_snapshot.T\n",
        "\n",
        "\n",
        "        CLE.append(np.zeros(NUM_OF_VERTICES))\n",
        "\n",
        "\n",
        "        FR_temp, FF_temp, MF_temp = matrix_sec(mat, vlist, sec_number = 10)\n",
        "\n",
        "        ## Friends (degrees)\n",
        "        # FR_temp = np.log(FR_temp+1)\n",
        "        if FR_temp.max()>0 and minmaxscale:\n",
        "            print(f'FR_max_{index}: {FR_temp.max()}')\n",
        "            FR_temp=FR_temp/FR_temp.max()\n",
        "        FR.append(FR_temp)\n",
        "\n",
        "        ## Friends of Friends (total shared neiboughrs)\n",
        "        # FF_temp = np.log(FF_temp+1)\n",
        "        if FF_temp.max()>0 and minmaxscale:\n",
        "            FF_temp=FF_temp/FF_temp.max()\n",
        "        FF.append(FF_temp)\n",
        "\n",
        "        # MF_temp = np.log(MF_temp+1)\n",
        "        if MF_temp.max()>0 and minmaxscale:\n",
        "            MF_temp = MF_temp/MF_temp.max()\n",
        "        MF.append(MF_temp)\n",
        "\n",
        "\n",
        "    FRmax = max([i.max() for i in FR])\n",
        "    FFmax = max([i.max() for i in FF])\n",
        "    MFmax = max([i.max() for i in MF])\n",
        "\n",
        "    \"\"\"\n",
        "    Computes hand-crafted properties for all vertices in vlist\n",
        "    \"\"\"\n",
        "\n",
        "    all_properties=[]\n",
        "    print_progress(('    Computed all matrix squares, ready to ruuuumbleeee...'),2,VERBOSE)\n",
        "\n",
        "    all_properties = np.zeros([len(vlist), len(all_sparse)*5], dtype='float32')\n",
        "    for ii in range(len(vlist)):\n",
        "        vals=compute_all_properties(all_sparse,\n",
        "                                    FR,\n",
        "                                    FF,\n",
        "                                    CLE,\n",
        "                                    MF,\n",
        "                                    ii,\n",
        "                                    vlist[ii][0],\n",
        "                                    vlist[ii][1])\n",
        "\n",
        "        # all_properties.append(vals)\n",
        "        all_properties[ii] = vals\n",
        "        if ii%10**5==0:\n",
        "            print_progress(('compute features: (',time.time()-time_start,'sec) ',ii/10**6,'M/',len(vlist)/10**6,'M'),2)\n",
        "            time_start=time.time()\n",
        "    print_progress('Feature generation completed.',1,VERBOSE)\n",
        "    return all_properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NZGLiYTgRxBL"
      },
      "outputs": [],
      "source": [
        "# Generate training batches\n",
        "\n",
        "def data_gen(data_train0, data_train1, batch_size=1000000):\n",
        "  data_train0 = np.array(data_train0)\n",
        "  data_train1 = np.array(data_train1)\n",
        "  y_batch = []\n",
        "  x_batch = []\n",
        "  for i in range(batch_size//2):\n",
        "      for output, data in enumerate([data_train0, data_train1]):\n",
        "          rndid = np.random.randint(0, len(data), 1)\n",
        "          x_batch.append(data[rndid][0])\n",
        "          y_batch.append(output)\n",
        "  return np.array(x_batch), np.array(y_batch)\n",
        "\n",
        "def data_gen_rnd(data_train0, data_train1, batch_size=1000000, ratio = 0.5):\n",
        "  data_train0 = np.array(data_train0)\n",
        "  data_train1 = np.array(data_train1)\n",
        "  y_batch = []\n",
        "  x_batch = []\n",
        "  for i in range(batch_size):\n",
        "      if random.random() > ratio:\n",
        "          rnd_output_id = 0\n",
        "      else:\n",
        "          rnd_output_id = 1\n",
        "      data = [data_train0, data_train1][rnd_output_id]\n",
        "      rndid = np.random.randint(0, len(data), 1)\n",
        "      x_batch.append(data[rndid][0])\n",
        "      y_batch.append(rnd_output_id)\n",
        "  return np.array(x_batch), np.array(y_batch)  \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "K7lNaCHOAHGT"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(test_calculated = False, minmaxscale = True, years_delta = 3, weight=3):\n",
        "    print_progress(f'Cutoff degree: {CUTOFF}', 1, VERBOSE)\n",
        "    print_progress(f'years_delta: {years_delta}', 1, VERBOSE)\n",
        "    # Training data: Predicting 2014 from 2011\n",
        "    train_dynamic_graph_sparse,data_edges_train,solution_train = create_training_data(full_dynamic_graph_sparse_2017,\n",
        "                                                                                              YEAR_START_TRAIN-years_delta,\n",
        "                                                                                              years_delta,\n",
        "                                                                                              edges_used=EDGES_USED,\n",
        "                                                                                              vertex_degree_cutoff=CUTOFF,\n",
        "                                                                                              filter_data=True,\n",
        "                                                                                              connected_ratio = raw_data_connected_ratio,\n",
        "                                                                                              weight = weight\n",
        "                                                                                      )\n",
        "\n",
        "    #######################\n",
        "    ############################\n",
        "    # Creating adjacency matrices\n",
        "\n",
        "    # Training matrices\n",
        "    year_start = YEAR_START_TRAIN\n",
        "    day_origin = date(1990,1,1)\n",
        "    years=[(year_start-years_delta) - i for i in range(number_of_years)]\n",
        "\n",
        "    train_sparse_mat=[]\n",
        "    for yy in years:\n",
        "        for month in [12,6][0:number_of_months]:\n",
        "            if month == 12:\n",
        "                day_curr=date(yy,month,31)\n",
        "            if month == 6:\n",
        "                day_curr=date(yy,month,30)\n",
        "            train_edges_curr=train_dynamic_graph_sparse[train_dynamic_graph_sparse[:,2]<(day_curr-day_origin).days]\n",
        "\n",
        "            adj_mat_sparse_curr = sparse.csr_matrix((np.ones(len(train_edges_curr)), (train_edges_curr[:,0], train_edges_curr[:,1])), shape=(NUM_OF_VERTICES,NUM_OF_VERTICES))\n",
        "            train_sparse_mat.append(adj_mat_sparse_curr)\n",
        "            print_progress((f'    Created Graph Matrix for {yy}-{month} with {adj_mat_sparse_curr.size} edges'),2,VERBOSE)\n",
        "\n",
        "\n",
        "    print_progress('    Shuffle train data...',2,VERBOSE)\n",
        "    x = np.arange(len(data_edges_train))\n",
        "    random.shuffle(x)\n",
        "\n",
        "    data_edges_train = data_edges_train[x]\n",
        "    solution_train = solution_train[x]\n",
        "\n",
        "\n",
        "    print_progress(('Training, connected  : ',sum(solution_train==1)),1,VERBOSE)\n",
        "    print_progress(('Training, unconnected: ',sum(solution_train==0),1,VERBOSE))\n",
        "\n",
        "    #################\n",
        "    # Creating the features\n",
        "\n",
        "    data_train=compute_all_properties_of_list(train_sparse_mat,data_edges_train, minmaxscale=minmaxscale)\n",
        "    print_progress(f'Train data features of {YEAR_START_TRAIN-years_delta} and train solutions of {YEAR_START_TRAIN} computed.',1,VERBOSE)\n",
        "\n",
        "    data_train0=[]\n",
        "    data_train1=[]\n",
        "    for ii in range(len(data_edges_train)):\n",
        "        if solution_train[ii]==1:\n",
        "            data_train1.append(data_train[ii])\n",
        "        else:\n",
        "            data_train0.append(data_train[ii])\n",
        "\n",
        "    data_train1 = np.array(data_train1, dtype='float32')\n",
        "    data_train0 = np.array(data_train0, dtype='float32')\n",
        "\n",
        "\n",
        "    print(f'Number of features: {len(data_train[0])}')\n",
        "    print_progress('All the train features for the ',2,VERBOSE)\n",
        "\n",
        "    NUM_OF_FEATURES = 5\n",
        "    data_train0_arr = np.array(data_train0)\n",
        "    data_train0_arr_reshaped = data_train0_arr.reshape(data_train0_arr.shape[0], time_snapshots, NUM_OF_FEATURES)\n",
        "    data_train1_arr = np.array(data_train1)\n",
        "    data_train1_arr_reshaped = data_train1_arr.reshape(data_train1_arr.shape[0], time_snapshots, NUM_OF_FEATURES)\n",
        "\n",
        "\n",
        "    del(data_train0_arr)\n",
        "    del(data_train1_arr)\n",
        "\n",
        "    xtrain,ytrain = data_gen_rnd(data_train0_arr_reshaped, data_train1_arr_reshaped, batch_size=BATCH_SIZE, ratio = ratio)\n",
        "    xtest,ytest = xtrain[0:1000],ytrain[0:1000]\n",
        "\n",
        "    del(data_train0_arr_reshaped)\n",
        "    del(data_train1_arr_reshaped)\n",
        "\n",
        "    return (xtrain,ytrain,xtest,ytest)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2AWNSUhTbvQ"
      },
      "source": [
        "## Model consistency experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vZlMssV6qPOX"
      },
      "outputs": [],
      "source": [
        "# LSTM Model\n",
        "def train_nn():\n",
        "    from keras.models import Sequential\n",
        "    from keras.layers import Dense\n",
        "    from keras.layers import LSTM\n",
        "    from keras import callbacks\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=400,\n",
        "                  return_sequences=True,\n",
        "                  input_shape=(None, NUM_OF_FEATURES,)))\n",
        "    model.add(LSTM(units=400,\n",
        "                  return_sequences=False,\n",
        "                  input_shape=(None, NUM_OF_FEATURES,)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC'])\n",
        "    # print(model.summary())\n",
        "    history = model.fit(xtrain[:,:,FMASK], ytrain, epochs=EPOCHS, batch_size=NN_BATCHSIZE,\n",
        "                  validation_data=(xtest[:,:,FMASK], ytest), verbose=VERBOSE)\n",
        "    return history, model\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trains and evaluates the LSTM model on each of the 18 datasets\n",
        "for current_experiment in range(18):\n",
        "\n",
        "    with open(file_names[current_experiment], 'rb') as f:\n",
        "        edges = pickle.load(f)\n",
        "    print(file_names[current_experiment])\n",
        "    full_dynamic_graph_sparse_2017, \\\n",
        "    unconnected_vertex_pairs_2017, \\\n",
        "    unconnected_vertex_pairs_solution, \\\n",
        "    YEAR_START_FILE, \\\n",
        "    YEARS_DELTA, \\\n",
        "    CUTOFF, \\\n",
        "    MIN_WEIGHT = edges\n",
        "\n",
        "\n",
        "    # day_curr=date(2017,12,31)\n",
        "    day_origin=date(1990,1,1)\n",
        "\n",
        "\n",
        "    # Horizon size experiment\n",
        "    history_list = []\n",
        "    VERBOSE = 2\n",
        "\n",
        "    history_list = []\n",
        "\n",
        "    connected_edges_stats = []\n",
        "    FMASK = [0,1,2,3,4]\n",
        "\n",
        "    YEAR_START_TEST = YEAR_START_FILE + YEARS_DELTA\n",
        "    YEAR_START_TRAIN = YEAR_START_FILE\n",
        "    TEST_CALCULATED = False\n",
        "    xtrain,ytrain,xtest,ytest = preprocess_data(test_calculated = False, minmaxscale = True, years_delta=YEARS_DELTA, weight=MIN_WEIGHT)\n",
        "\n",
        "    NUM_OF_FEATURES = 5\n",
        "    for train_num in range(3):\n",
        "        hist, model = train_nn()\n",
        "        history_list.append([hist])\n",
        "\n",
        "    hist_array = np.array([[max(history_list[i][0].history['auc']),max(history_list[i][0].history['val_auc'])] for i in range(len(history_list))])\n",
        "    hist_max = hist_array.max(axis=0)\n",
        "\n",
        "    # Create properties for evaluation\n",
        "    year_start = YEAR_START_FILE\n",
        "    full_dynamic_graph_sparse = full_dynamic_graph_sparse_2017\n",
        "    unconnected_vertex_pairs = unconnected_vertex_pairs_2017\n",
        "\n",
        "    print('2) Makes predictions for '+str(year_start)+' -> '+str(year_start+YEARS_DELTA)+' data.')\n",
        "    years=[year_start,year_start-1,year_start-2]#,year_start-4,year_start-7,year_start-12]\n",
        "\n",
        "    print('2.1) Computes the 15 properties for the '+str(year_start)+' data.')\n",
        "    eval_sparse=[]\n",
        "    for yy in years:\n",
        "        print('    Create Graph for ', yy)\n",
        "        day_curr=date(yy,12,31)\n",
        "        eval_edges_curr=full_dynamic_graph_sparse[full_dynamic_graph_sparse[:,2]<(day_curr-day_origin).days]\n",
        "        adj_mat_sparse_curr = sparse.csr_matrix(\n",
        "                                                (np.ones(len(eval_edges_curr)), (eval_edges_curr[:,0], eval_edges_curr[:,1])),\n",
        "                                                shape=(NUM_OF_VERTICES,NUM_OF_VERTICES)\n",
        "                                              )\n",
        "\n",
        "        eval_sparse.append(adj_mat_sparse_curr)\n",
        "\n",
        "    print('    compute all properties for evaluation')\n",
        "    eval_examples=compute_all_properties_of_list(eval_sparse,unconnected_vertex_pairs)\n",
        "    eval_examples=np.array(eval_examples)\n",
        "    eval_examples_org=eval_examples \n",
        "\n",
        "    eval_reshaped = eval_examples_org.reshape(eval_examples_org.shape[0], 3, 5)\n",
        "\n",
        "    scores = model.evaluate(eval_reshaped, unconnected_vertex_pairs_solution, verbose=0)\n",
        "\n",
        "    with open(f'logs_{current_experiment+1}.txt', 'a') as myfile:\n",
        "        myfile.write(f'File name: {file_names[current_experiment]}' + '\\n')\n",
        "        myfile.write(f'Validation AUC {scores[1]}' + '\\n')\n",
        "        myfile.write(f'Validatin Size {len(unconnected_vertex_pairs_solution)}' + '\\n')\n",
        "        myfile.write(f'Validation positives: {unconnected_vertex_pairs_solution.sum()}' + '\\n')\n",
        "        myfile.write(f'YEAR_START: {YEAR_START_FILE}' + '\\n')\n",
        "        myfile.write(f'YEARS_DELTA: {YEARS_DELTA}' + '\\n')\n",
        "        myfile.write(f'CUTOFF: {CUTOFF}' + '\\n')\n",
        "        myfile.write(f'MIN_WEIGHT {MIN_WEIGHT}' + '\\n')\n",
        "        myfile.write(f'Train AUC {hist_max[0]}' + '\\n')\n",
        "        myfile.write(f'Test AUC {hist_max[1]}' + '\\n')\n",
        "        myfile.write(f'Train Size {len(ytrain)}' + '\\n')\n",
        "        myfile.write(f'Train positives {ytrain.sum()}' + '\\n')  "
      ],
      "metadata": {
        "id": "WCsD4LZAmiiP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}